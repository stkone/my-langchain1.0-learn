## ä¸€ã€Model æ¨¡å‹

LangChainæ”¯æŒçš„æ¨¡å‹æœ‰ä¸‰å¤§ç±»

- 1.å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ ï¼Œä¹Ÿå«éå¯¹è¯æ¨¡å‹

  ä¹Ÿå«æ–‡æœ¬è¡¥å…¨æ¨¡å‹ï¼Œæ¥æ”¶æ–‡æœ¬å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªè¡¥å…¨å­—ç¬¦ä¸²ä½œä¸ºè¾“å‡ºã€‚

  ä¾‹å¦‚ç»™å®šä¸€ä¸ªæç¤ºâ€œä»Šå¤©çš„å¤©æ°”å¦‚ä½•?â€æ¨¡å‹ä¼šç”Ÿæˆä¸€ä¸ªç›¸åº”çš„ç­”æ¡ˆâ€œä»Šå¤©çš„å¤©æ°”å¾ˆå¥½ã€‚â€

  å¯ä»¥ç†è§£ä¸ºåªèƒ½ä¸€é—®ä¸€ç­”ã€‚

- 2.èŠå¤©æ¨¡å‹ï¼ˆChat Modelï¼‰ï¼Œä¹Ÿå«å¯¹è¯æ¨¡å‹

  ä¸»è¦ä»£è¡¨Open AIçš„ChatGPTç³»åˆ—æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šå¸¸ç”±è¯­è¨€æ¨¡å‹æ”¯æŒï¼Œä½†å®ƒä»¬çš„APIæ›´åŠ ç»“æ„åŒ–ã€‚èŠå¤©æ¨¡å‹åŒ…è£…å™¨ä¸º ChatOpenAlã€‚

  å…·ä½“æ¥è¯´ï¼ŒChatOpenAlå°†ä¸€ç³»åˆ—çš„èŠå¤©æ¶ˆæ¯/èŠå¤©æ¶ˆæ¯åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›èŠå¤©æ¶ˆæ¯ã€‚

  ä»£ç å¦‚ä¸‹ï¼š

  ```python
  # å°è£…openaiè§„èŒƒçš„æ¨¡å‹å¯¹è±¡
  # model = ChatOpenAI(api_key=os.getenv("DASHSCOPE_API_KEY"), model="qwen-max",base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
  # ä¸æ˜¯å…¼å®¹openAIçš„æ¨¡å‹ï¼Œä½¿ç”¨init_chat_modelåˆ›å»º ,å…¼å®¹ä»»ä½•çš„å¤§æ¨¡å‹
  # å¦‚æœç¡®å®šæ¨¡å‹æ˜¯å…¼å®¹openaiçš„ï¼Œå¦‚æœæ¨¡å‹åˆ‡æ¢æ¯”è¾ƒé¢‘ç¹ï¼Œå»ºè®®ä½¿ç”¨init_chat_modelåˆ›å»º
  # model = init_chat_model(model="deepseek-chat", model_provider="deepseek")
  # ç¡®å®šä½¿ç”¨æŸä¸€ä¸ªå¹³å°çš„æ¨¡å‹
  model = ChatTongyi()
  # ChatDeepSeek()
  ```

- 3.æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆEmbedding Modelï¼‰

  åœ¨Ragéƒ¨åˆ†å…·ä½“ä»‹ç»å®ç°

## äºŒã€æç¤ºè¯æ¨¡æ¿

1.  PromptTemplateï¼šå¸¸ç”¨çš„Stringæç¤ºæ¨¡æ¿

2. èŠå¤©æç¤ºæ¨¡æ¿ ChatPromptTemplateï¼š å¸¸ç”¨çš„Chatæç¤ºæ¨¡æ¿ï¼Œç”¨äºç»„åˆå„ç§è§’è‰²çš„æ¶ˆæ¯æ¨¡æ¿ï¼Œä¼ å…¥èŠå¤©æ¨¡å‹ã€‚æ¶ˆæ¯æ¨¡æ¿åŒ…æ‹¬ï¼šChatMessagePromptTemplateã€HumanMessagePromptTemplateã€AIlMessagePromptTemplateã€SystemMessagePromptTemplateç­‰

   ```python
   from langchain_community.chat_models import ChatTongyi
   from langchain_core.output_parsers import StrOutputParser
   from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, 
       HumanMessagePromptTemplate, AIMessagePromptTemplate
   
   # æ¨¡å‹å®¢æˆ·ç«¯
   model = ChatTongyi()
   
   # æç¤ºè¯æ¨¡æ¿
   # prompt = PromptTemplate(
   #     template="ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å¸®æˆ‘æŠŠä¸€ä¸‹å†…å®¹ç¿»è¯‘æˆ{language}ï¼š{text}"
   #     , input_variables=["text", "language"]
   # )
   
   # å¯¹è¯æ¨¡å‹ï¼Œè§’è‰²çš„è®¾ç½®ï¼Œå¤šè½®å¯¹è¯è®°å¿†ç»´æŒ
   # æç¤ºè¯è§’è‰²ï¼šsystem å…¨å±€ï¼Œç»Ÿä¸€è®¾ç½®   user ç”¨æˆ·æ¯æ¬¡æé—®  assistant å¤§æ¨¡å‹å›å¤ï¼Œå¹³å¸¸ä¸Šä¸‹æ–‡å­˜å‚¨
   
   prompt = ChatPromptTemplate.from_messages([
       # ç³»ç»Ÿæç¤ºè¯
       SystemMessagePromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œå¯ä»¥ç¿»è¯‘ä»»ä½•ä¸€ç§è¯­è¨€"),
       # ç”¨æˆ·æç¤ºè¯
       HumanMessagePromptTemplate.from_template("è¯·å°†ä¸€ä¸‹å†…å®¹ç¿»è¯‘æˆ{language}ï¼š{text}"),
       # åŠ©æ‰‹æç¤ºè¯
       # AIMessagePromptTemplate.from_template("{text}"),
   ])
   
   # ç»™å‚æ•°èµ‹å€¼
   # fact = prompt.format(text="hello world", language="ä¸­æ–‡")
   # fact2 = prompt.format(text="æˆ‘çˆ±ä½ ", language="æ—¥æ–‡")
   
   # æ‰§è¡Œï¼Œæ‹¿ç»“æœ
   # result = model.invoke(fact)
   # result2 = model.invoke(fact2)
   # print(result)
   # print(result2)
   
   # ç»“æœè§£æå™¨,æ ¼å¼åŒ–è¾“å‡º
   out = StrOutputParser()
   
   # print(out.invoke(result))
   
   # åº•å±‚è¿˜æ˜¯å‡½æ•°è°ƒä»¤ï¼Œç®€åŒ–ä¹¦å†™ï¼Œå›ºå®šçš„æµç¨‹çš„è°ƒç”¨ï¼Œæ ¼å¼åŒ–
   chain = prompt | model | out
   
   print(chain.invoke({"text": "hello world", "language": "ä¸­æ–‡"}))
   ```

3. æ ·æœ¬æç¤ºæ¨¡æ¿ FewShotPromptTemplateï¼šé€šè¿‡å°‘é‡æ ·æœ¬æ¥æ•™æ¨¡å‹å¦‚ä½•å›ç­”

```python
#å°‘æ ·æœ¬æç¤ºæ¨¡ç‰ˆçš„ä½¿ç”¨
from langchain.prompts import PromptTemplate
from langchain.prompts.few_shot import FewShotPromptTemplate
from models import get_lc_model_client

#è·å¾—è®¿é—®å¤§æ¨¡å‹å®¢æˆ·ç«¯
client = get_lc_model_client()

# åˆ›å»ºç¤ºä¾‹
examples = [
    {"sinput": "2+2", "soutput": "4", "sdescription": "åŠ æ³•è¿ç®—"},
    {"sinput": "5-2", "soutput": "3", "sdescription": "å‡æ³•è¿ç®—"},
]

# é…ç½®ä¸€ä¸ªæç¤ºæ¨¡æ¿ï¼Œç”¨æ¥ä¸€ä¸ªç¤ºä¾‹æ ¼å¼åŒ–
examples_prompt_tmplt_txt = "ç®—å¼ï¼š {sinput} å€¼ï¼š {soutput} ç±»å‹ï¼š {sdescription} "

# è¿™æ˜¯ä¸€ä¸ªæç¤ºæ¨¡æ¿çš„å®ä¾‹ï¼Œç”¨äºè®¾ç½®æ¯ä¸ªç¤ºä¾‹çš„æ ¼å¼
prompt_sample = PromptTemplate.from_template(examples_prompt_tmplt_txt)

# åˆ›å»ºå°‘æ ·æœ¬ç¤ºä¾‹çš„å¯¹è±¡
prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=prompt_sample,
    prefix="ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶, èƒ½å¤Ÿå‡†ç¡®è¯´å‡ºç®—å¼çš„ç±»å‹ï¼Œ",
    suffix="ç°åœ¨ç»™ä½ ç®—å¼: {input} ï¼Œ å€¼: {output} ï¼Œå‘Šè¯‰æˆ‘ç±»å‹ï¼š",
    input_variables=["input", "output"]
)
print(prompt.format(input="2*5", output="10"))  # ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶,ç®—å¼: 2*5  å€¼: 10

print('-' * 50)

result = client.invoke(prompt.format(input="2*5", output="10"))
print(result.content)  # ä½¿ç”¨: ä¹˜æ³•è¿ç®—
```

## ä¸‰ã€è¾“å‡ºè§£æå™¨

è¾“å‡ºè§£æå™¨è´Ÿè´£è·å– LLM çš„è¾“å‡ºå¹¶å°†å…¶è½¬æ¢ä¸ºæ›´åˆé€‚çš„æ ¼å¼ã€‚

LangChainæœ‰è®¸å¤šä¸åŒç±»å‹çš„è¾“å‡ºè§£æå™¨ï¼šCommaSeparatedListOutputParserã€DatetimeOutputParserã€JsonOutputParserã€XMLOutputParserç­‰ç­‰

1.CommaSeparatedListOutputParser ï¼šè¿”å›åˆ—è¡¨å½¢å¼ï¼Œç”¨é€—å·åˆ†éš”è¿”å›ç»“æœ

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import CommaSeparatedListOutputParser, StrOutputParser
from models import get_lc_model_client

# è·å¾—è®¿é—®å¤§æ¨¡å‹å®¢æˆ·ç«¯
client = get_lc_model_client()

# åˆ›å»ºè§£æå™¨
# output_parser = StrOutputParser()
output_parser = CommaSeparatedListOutputParser()

# æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¨‹åºå‘˜"),
    ("user", "{input}")
])

# å°†æç¤ºå’Œæ¨¡å‹åˆå¹¶ä»¥è¿›è¡Œè°ƒç”¨
chain = prompt | client | output_parser

# ç¤ºä¾‹è°ƒç”¨
#æ˜ç¡®å‘Šè¯‰å¤§æ¨¡å‹ç”¨ç”¨é€—å·åˆ†éš”è¿”å›ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨CommaSeparatedListOutputParserè·å¾—å†…å®¹åä»¥
# åˆ—è¡¨çš„å½¢å¼è·å¾—ä»¥è¿›è¡Œåç»­å¤„ç†ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²
print(chain.invoke({"input": "åˆ—å‡ºPythonçš„ä¸‰ä¸ªä¸»è¦ç‰ˆæœ¬, ç”¨é€—å·åˆ†éš”"}))
print(chain.invoke({"input": "åˆ—ä¸¾ä¸‰ä¸ªå¸¸è§çš„æœºå™¨å­¦ä¹ æ¡†æ¶, ç”¨é€—å·åˆ†éš”"}))
```

2.DatetimeOutputParser æŒ‰ç…§æ—¥æœŸæ ¼å¼è¿”å›ç»“æœ

```python
from langchain.output_parsers import DatetimeOutputParser
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

from models import get_lc_model_client

#è·å¾—è®¿é—®å¤§æ¨¡å‹å®¢æˆ·ç«¯
client = get_lc_model_client()

# å®šä¹‰æ¨¡æ¿æ ¼å¼
template = """
å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š{question}

{format_instructions}
"""

# ä½¿ç”¨æ—¥æœŸæ—¶é—´è§£æå™¨
output_parser = DatetimeOutputParser()

prompt = PromptTemplate.from_template(
    template,
    # æŒ‡å®šæ—¥æœŸçš„æ ¼å¼ç±»å‹  yyyy-MM-dd hh:mm:ss
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
)
print(prompt)
print("-"*30)
print(output_parser.get_format_instructions())
print("-"*30)
# é“¾å¼è°ƒç”¨
chain = prompt | client | output_parser
output = chain.invoke({"question": "æ–°ä¸­å›½æ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ"})
# æ‰“å°è¾“å‡º
print(output)  # 1949-10-01

# æ‰§è¡Œ
# output = model.invoke(prompt.format(question='æ–°ä¸­å›½æˆç«‹çš„æ—¶é—´ï¼Ÿ'))
# datetime_parsed = output_parser.parse(output.content)
# # æ‰“å°è¾“å‡º
# print(datetime_parsed)  # 1949-10-01
```

3.JsonOutputParser è¡¨ç¤ºå°†ç»“æœä»¥jsonæ ¼å¼è¿”å›

```python
from langchain_core.prompts import ChatPromptTemplate
# åˆ›å»ºè§£æå™¨
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser

from models import get_lc_model_client

#è·å¾—è®¿é—®å¤§æ¨¡å‹å®¢æˆ·ç«¯
client = get_lc_model_client()

# output_parser = StrOutputParser()
output_parser = JsonOutputParser()

# æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¨‹åºå‘˜"),
    ("user", "{input}")
])

# å°†æç¤ºå’Œæ¨¡å‹åˆå¹¶ä»¥è¿›è¡Œè°ƒç”¨
chain = prompt | client | output_parser


#æ˜ç¡®å‘Šè¯‰å¤§æ¨¡å‹ç”¨JSONæ ¼å¼è¿”å›ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨JSONOutputParserè·å¾—JSONæ ¼å¼çš„å†…å®¹ä»¥è¿›è¡Œåç»­å¤„ç†ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²
result = chain.invoke({"input": "langchainæ˜¯ä»€ä¹ˆ? é—®é¢˜ç”¨question å›ç­”ç”¨ans è¿”å›ä¸€ä¸ªJSONæ ¼å¼"})
print(type(result))
print(result)
# print(chain.invoke({"input": "å¤§æ¨¡å‹ä¸­çš„langchainæ˜¯ä»€ä¹ˆ?"}))
```

## å››ã€Langchainçš„é“¾å’ŒLCEL

å®é™…ä¸Šï¼ŒLangChainçš„åå­—æºè‡ªå…¶æ¡†æ¶çš„æ ¸å¿ƒè®¾è®¡æ€è·¯:

ç”¨é“¾(Chain)ï¼Œå°†å¤§è¯­è¨€æ¨¡å‹å¼€å‘çš„å„ä¸ªç»„ä»¶é“¾æ¥èµ·æ¥ï¼Œä»¥æ„å»ºå¤æ‚çš„åº”ç”¨ç¨‹åºã€‚

é“¾çš„ä¸»è¦åŠŸèƒ½æ˜¯ç®¡ç†åº”ç”¨ç¨‹åºä¸­çš„æ•°æ®æµåŠ¨ï¼Œå®ƒå°†ä¸åŒçš„ç»„ä»¶(æˆ–å…¶ä»–é“¾)é“¾æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ã€‚æ¯ä¸ªç»„ä»¶éƒ½æ˜¯é“¾ä¸­çš„ä¸€ä¸ªç¯èŠ‚ï¼Œå®ƒä»¬æŒ‰ç…§é¢„è®¾çš„é¡ºåºï¼Œæ¥åŠ›å®Œæˆå„è‡ªçš„ä»»åŠ¡ï¼Œå°±å¥½æ¯”å·¥å‚é‡Œçš„æµæ°´çº¿ã€‚

LangChainæ—©æœŸçš„é“¾åªæœ‰ä¸€ç§å®ç°æ–¹å¼ï¼Œä½†æ˜¯åæ¥LangChainå¼•å…¥äº†LCELé“¾é€æ¸ä»£æ›¿åŸæœ‰çš„å®ç°ã€‚

æ‰€ä»¥åœ¨V0.1ä»¥åï¼Œå¼€å§‹å¯¹é“¾è¿›è¡Œäº†åˆ†ç±»ï¼Œåˆ†ä¸ºäº†é—ç•™é“¾å’ŒLCELé“¾ã€‚åœ¨LangChainå®˜æ–¹å°†æ‰€æœ‰çš„æ—§é“¾å…¨éƒ¨æ›¿æ¢ä¸ºLCELé“¾ä¹‹å‰ï¼Œé—ç•™é“¾æš‚æ—¶å¯ç”¨ã€‚

LCEL ä¹Ÿè¢«ç§°ä¸ºLangChainè¡¨è¾¾å¼(LangChain Expression Language)ï¼Œæ˜¯ä¸€ç§ç”¨å£°æ˜å¼çš„æ–¹æ³•æ¥é“¾æ¥LangChainç»„ä»¶ã€‚

æ‰€æœ‰å¯ä»¥è¢«é“¾èµ·æ¥çš„ç»„ä»¶ï¼Œå¦‚å¤§è¯­è¨€æ¨¡å‹ã€è¾“å‡ºè§£æå™¨ã€æ£€ç´¢å™¨ã€æç¤ºè¯æ¨¡æ¿ç­‰éƒ½æ”¯æŒä¸‹é¢ä¸‰ä¸ªæ–¹æ³•ï¼š

stream: æµå¼è¿”å›å“åº”çš„å—

invoke: æ¥å—è¾“å…¥è¿”å›è¾“å‡º

batch: æ¥å—æ‰¹é‡è¾“å…¥è¿”å›è¾“å‡ºåˆ—è¡¨

æ‰€ä»¥æœ€ç»ˆç»„æˆçš„é“¾åœ¨ä½¿ç”¨ä¸Šï¼Œä¹Ÿæ˜¯è°ƒç”¨è¿™ä¸‰ä¸ªæ–¹æ³•ã€‚

### 1.é“¾çš„åŸºæœ¬ä½¿ç”¨

å¦‚ä½•æŠŠç»„ä»¶é“¾èµ·æ¥ï¼Ÿ

æœ€ç®€å•æœ€å¸¸è§çš„å°±æ˜¯ç”¨ç®¡é“æ“ä½œç¬¦ â€œ | â€ï¼Œåœ¨å‰é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å·²ç»å¤šæ¬¡ä½¿ç”¨è¿‡äº†

```python
#è·å¾—è®¿é—®å¤§æ¨¡å‹å®¢æˆ·ç«¯
client = get_lc_model_client()

# output_parser = StrOutputParser()
output_parser = JsonOutputParser()

# æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¨‹åºå‘˜"),
    ("user", "{input}")
])

# é€šè¿‡ç®¡é“ç¬¦ | å®ç°é“¾çš„ç»„åˆ
chain = prompt | client | output_parser


#æ˜ç¡®å‘Šè¯‰å¤§æ¨¡å‹ç”¨JSONæ ¼å¼è¿”å›ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨JSONOutputParserè·å¾—JSONæ ¼å¼çš„å†…å®¹ä»¥è¿›è¡Œåç»­å¤„ç†ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²
result = chain.invoke({"input": "langchainæ˜¯ä»€ä¹ˆ? é—®é¢˜ç”¨question å›ç­”ç”¨ans è¿”å›ä¸€ä¸ªJSONæ ¼å¼"})
```

### 2.é“¾çš„å¸¸ç”¨ç»„ä»¶ï¼š

#### RunnableSequence é¡ºåºæ‰§è¡Œé“¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ï¼Œç±»ä¼¼|ç®¡é“ç¬¦

```python
import os

#  debugæ¨¡å¼çš„åŒ…
import langchain
from langchain.chat_models import init_chat_model
from langchain_community.chat_models import ChatTongyi, ChatHunyuan
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, 
    HumanMessagePromptTemplate, AIMessagePromptTemplate
from langchain_core.runnables import RunnableSequence
from langchain_openai import ChatOpenAI

# é“¾å¼è°ƒç”¨
# 1.åˆ›å»ºæ¨¡å‹å¯¹è±¡ï¼ŒåŸºäºOpenAI è§„èŒƒ
# ä½œç”¨ï¼šè·Ÿè¸ªé“¾è·¯ï¼Œè·Ÿè¸ªæ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥ï¼Œè¾“å‡º     è°ƒè¯•ç¨‹åºæ—¶ä½¿ç”¨
# langchain.debug=True


model = ChatTongyi()


# 2.æ„å»ºæç¤ºè¯
# æç¤ºè¯æ¨¡æ¿  ctrl+é¼ æ ‡å·¦é”®
prompt = PromptTemplate(
    template="ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å¸®æˆ‘æŠŠä¸€ä¸‹å†…å®¹ç¿»è¯‘æˆ{language}ï¼š{text}"
    , input_variables=["text", "language"]
)


# æ ¼å¼åŒ–è¿”å›ç»“æœ,å­—ç¬¦ä¸²æ ¼å¼åŒ–,contentå†…å®¹æå–
out = StrOutputParser()

# æ„å»ºé“¾   |ç®¡é“    linuxå‘½ä»¤ä¸­æœ‰ç®¡é“,langchainæ¡†æ¶é¢„åˆ¶äº†å¾ˆå¤šå·¥ä½œé“¾
# chain = prompt | model | out
chain = RunnableSequence(prompt, model, out)

# é“¾çš„è°ƒç”¨
result = chain.invoke({"language": "è‹±æ–‡", "text": "æˆ‘å–œæ¬¢ç¼–ç¨‹"})

print(result)
```

#### RunnableLambda

å¾ˆå¤šçš„æ—¶å€™LangChainç»„ä»¶æœªæä¾›çš„åŠŸèƒ½ï¼Œéœ€è¦æˆ‘ä»¬è‡ªå·±å†™å‡½æ•°å®ç°ï¼Œä½†æ˜¯è¿™ä¸ªå‡½æ•°æˆ‘ä»¬ä¹Ÿéœ€è¦æŠŠå®ƒåŠ å…¥é“¾ï¼Œä½œä¸ºå·¥ä½œæµç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œæ­¤æ—¶å°±éœ€è¦ä½¿ç”¨RunnableLambda,è£…é¥°å™¨@chainæ˜¯RunnableLambdaçš„å¦ä¸€ç§å†™æ³•

```python
#RunnableLambdaçš„ä½¿ç”¨
from operator import itemgetter
import langchain
from langchain_community.chat_models import ChatOllama, ChatHuggingFace
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables import chain
from models import get_lc_model_client

#å¼€å¯è¯¥å‚æ•°ï¼Œä¼šè¾“å‡ºè°ƒè¯•ä¿¡æ¯
# langchain.debug = True
#è·å¾—è®¿é—®å¤§æ¨¡å‹å®¢æˆ·ç«¯
client = get_lc_model_client()

#  langchainè°ƒç”¨æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹
# client = ChatOllama()
# ChatHuggingFace()


#å®šä¹‰æç¤ºæ¨¡ç‰ˆ
chat_template = ChatPromptTemplate.from_template(" {a} + {b}æ˜¯å¤šå°‘ï¼Ÿ")


output = StrOutputParser()
#è·å¾—å­—ç¬¦ä¸²çš„é•¿åº¦ æ²¡æœ‰ç»§æ‰¿Runnableç±»
def length_function(text):
    return len(text)

#å°†ä¸¤ä¸ªå­—ç¬¦ä¸²é•¿åº¦çš„æ•°é‡ç›¸ä¹˜
def _multiple_length_function(text1, text2):
    return len(text1) * len(text2)

#@chainæ˜¯RunnableLambdaçš„å¦ä¸€ç§å†™æ³•
@chain
def multiple_length_function(_dict):
    return _multiple_length_function(_dict["text1"], _dict["text2"])

# chain1 = chat_template | client |output
chain1 = chat_template | client

#ä½¿ç”¨RunnableLambdaå°†å‡½æ•°è½¬æ¢ä¸ºä¸LCELå…¼å®¹çš„ç»„ä»¶

# RunnableLambda è°ƒç”¨è‡ªå®šä¹‰å‡½æ•°ï¼Œè‡ªå®šä¹‰å‡½æ•°çš„åŠŸèƒ½å¯ä»¥å…¶ä»–å·¥å…·è¿›è¡Œå‚æ•°çš„è·å–
chain2 = (
    # a=3 b=12
    {
         "a": itemgetter("foo") | RunnableLambda(length_function),
        # "a": itemgetter("foo") | length_function,
        "b": {"text1": itemgetter("foo"), "text2": itemgetter("bar")}| multiple_length_function,
    }
    | chain1
)
print(chain2.invoke({"foo": "abc", "bar": "abcd"}))

#æ¨¡æ‹Ÿç”¨æˆ·çš„ä¸šåŠ¡ï¼Œå¯ä»¥ä»æ•°æ®åº“ã€å…¶ä»–æ–‡ä»¶ä¸­è·å¾—æ•°æ®

# itemgetter æ ¹æ®keyè·å–å­—å…¸ä¸­çš„å€¼   a=3 b=12
chain3 = (
          {
             "a": ( itemgetter("foo") | RunnableLambda(length_function) ),
             "b": ( {"text1": itemgetter("foo"), "text2": itemgetter("bar")}| multiple_length_function )
          }
    | chain1  )| output
print(chain3.invoke({"foo": "abc", "bar": "abcd"}))
```

#### RunnableParallel

æŠŠç»„ä»¶é“¾èµ·æ¥ï¼Œå‰é¢æˆ‘ä»¬ä½¿ç”¨äº†RunnableSequenceã€‚Sequenceçš„ä¸­æ–‡æ„æ€â€œé¡ºåºï¼Œä¸€è¿ä¸²â€ï¼Œä¹Ÿå°±æ˜¯æ‰€æœ‰çš„ç»„ä»¶æˆ–å¤šä¸ªä»»åŠ¡éƒ½æ˜¯æŒ‰é¡ºåºä¸€ä¸ªä¸ªæ‰§è¡Œçš„ã€‚

ä½†æ˜¯æœ‰æ—¶å€™ï¼Œæˆ‘ä»¬å¸Œæœ›å¤šä¸ªä»»åŠ¡èƒ½å¤ŸåŒæ—¶æ‰§è¡Œï¼Œè¿™æ—¶å¯ä»¥ä½¿ç”¨RunnableParallel

```python
#RunnableParallelçš„ä½¿ç”¨
from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnableMap, RunnableSequence


def add_one(x: int) -> int:
    return x + 1

def mul_two(x: int) -> int:
    return x * 2

def mul_three(x: int) -> int:
    return x * 3

# æµ‹è¯•RunnableParallel, RunnableMap å¹¶è¡Œæ‰§è¡Œ
# chain = RunnableSequence(add_one,mul_two,mul_three)    12 åŸå› ï¼šRunnableSequence å°è£…æˆäº†Runnableå®ç°
# print(chain.invoke(1))

# è¡¨ç¤º add_one æ‰§è¡Œçš„åŒæ—¶ mul_twoï¼Œmul_three ä¸¤ä¸ªéƒ½ä¼šæ‰§è¡Œ è¿”å›å€¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé€šè¿‡key
# chain = RunnableParallel(
#     a =add_one,
#     b = mul_two,
#     c = mul_three
# )

# chain = RunnableMap(
#     a = add_one,
#     b = mul_two,
#     c = mul_three
# )
# # å­—å…¸ç»“æ„ {'a': 3, 'b': 4, 'c': 6}   java ä¸­map
# print(chain.invoke(2))

chain1 = RunnableLambda(add_one)

chain2 = chain1|RunnableParallel(
    a = mul_two,
    b = mul_three
)
# ç»“æœå­—å…¸ï¼š{'a': 6, 'b': 9}
print(chain2.invoke(2))
```

#### RunnablePassthrough

RunnablePassthroughç±»å…è®¸æˆ‘ä»¬åœ¨LangChainçš„é“¾ä¸­ä¼ é€’æ•°æ®ï¼š

1ã€ç¡®ä¿æ•°æ®ä¿æŒä¸å˜ï¼Œç›´æ¥ç”¨äºåç»­æ­¥éª¤çš„è¾“å…¥ã€‚ RunnablePassthroughå¸¸ç”¨åœ¨é“¾çš„ç¬¬ä¸€ä¸ªä½ç½®ï¼Œç”¨äºæ¥æ”¶ç”¨æˆ·çš„è¾“å…¥ï¼ˆä¹Ÿå¯ä»¥ç”¨åœ¨ä¸­é—´ä½ç½®ï¼Œåˆ™ç”¨äºæ¥æ”¶ä¸Šä¸€æ­¥çš„è¾“å‡ºï¼‰ã€‚

2ã€RunnablePassthroughä¹Ÿå…è®¸é€šè¿‡assignå¯¹æ•°æ®å¢å¼ºåï¼Œå†å¾€åä¼ ã€‚

é€šè¿‡ `RunnablePassthrough`ï¼Œä½ å¯ä»¥åœ¨ LangChain ä¸­ä»¥æœ€å°çš„æˆæœ¬å®ç°æ•°æ®å¢å¼ºï¼Œè®©æµç¨‹ä¿æŒç®€æ´çš„åŒæ—¶å…·å¤‡çµæ´»çš„æ‰©å±•èƒ½åŠ›ï¼Œå°¤å…¶é€‚åˆéœ€è¦è½»é‡çº§æ•°æ®å¤„ç†çš„åœºæ™¯ï¼ˆå¦‚æ—¥å¿—ã€å…ƒæ•°æ®æ·»åŠ ã€ä¸´æ—¶å‚æ•°æ³¨å…¥ï¼‰

```python
#RunnablePassthroughçš„ä½¿ç”¨
#RunnablePassthroughçš„ä¸¤ç§ç”¨æ³•éƒ½å°†åœ¨æˆ‘ä»¬åé¢çš„è¯¾ç¨‹ä¸­çœ‹åˆ°
from langchain_core.runnables import RunnableParallel, RunnablePassthrough


# RunnablePassthroughåŸæ ·è¿›è¡Œæ•°æ®ä¼ é€’
# runnable = RunnableParallel(
#     passed=RunnablePassthrough(),
#     modified=lambda x: x["num"] + 1,
# )
# #
# print(runnable.invoke({"num": 1}))

# RunnablePassthroughå¯¹æ•°æ®å¢å¼ºåä¼ é€’
#RunnablePassthrough().assignå®ƒä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å­—å…¸ï¼ŒåŒ…å«åŸå§‹çš„æ‰€æœ‰å­—æ®µä»¥åŠä½ æ–°æŒ‡å®šçš„å­—æ®µã€‚
runnable = RunnableParallel(
    passed=RunnablePassthrough().assign(query=lambda x: x["num"] + 2),
    modified=lambda x: x["num"] + 1,
)
# {'passed': {'num': 1, 'query': 3}, 'modified': 2}
print(runnable.invoke({"num": 1}))
```

## äº”ã€è®°å¿†ï¼ˆMemoryï¼‰

å¤§æ¨¡å‹æ˜¯æ— çŠ¶æ€çš„ï¼Œä¹Ÿå°±æ˜¯å¤§æ¨¡å‹æ˜¯è®°ä¸ä½æˆ‘ä»¬èŠå¤©ä¸­æ¯æ¬¡çš„å¯¹è¯å†…å®¹çš„ã€‚

éªŒè¯å¤§æ¨¡å‹çš„æ— çŠ¶æ€ï¼š

```python
#å±•ç¤ºå¤§æ¨¡å‹çš„æ— çŠ¶æ€ï¼Œè®°ä¸ä½æˆ‘ä»¬èŠå¤©ä¸­æ¯æ¬¡çš„å¯¹è¯å†…å®¹
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate

from models import get_lc_model_client
# ç›´æ¥è®¿é—®LLM
client = get_lc_model_client()

chat_template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template("ä½ æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹"),
        ('human', '{text}')
    ]
)
parser = StrOutputParser()

chain = chat_template | client | parser

while True:
    user_input = input("è¯·è¾“å…¥ 'quit' é€€å‡ºç¨‹åº: ")
    if user_input == 'quit':
        print("ç¨‹åºç»“æŸã€‚")
        break
    else:
        print(chain.invoke({'text': user_input}))

# # é—®é¢˜1
# print(chain.invoke({'text': 'ä½ å¥½ï¼Œæˆ‘æ˜¯å¤§ç™½'}))
# # é—®é¢˜2
# print(chain.invoke({'text': 'ä½ å¥½ï¼Œæˆ‘æ˜¯è°ï¼Ÿ'}))
```

#### ChatMessageHistoryçš„ä½¿ç”¨

ç”¨æ¥åœ¨å†…å­˜ä¸­å­˜å‚¨å¯¹è¯çš„ä¸Šä¸‹æ–‡ï¼Œä¹Ÿå°±æ˜¯ç»´æŒå¤šè½®å¯¹è¯çš„â€œè®°å¿†â€

ChatMessageHistoryæ¯”è¾ƒæ—©æœŸçš„å‘½å ï¼ŒåŠŸèƒ½å®ç°æ˜¯InMemoryChatMessageHistory

å¸¸ç”¨çš„APIæœ‰å››ä¸ªï¼š

add_user_messageå­˜ç”¨æˆ·æ¶ˆæ¯ 

add_ai_message å­˜AIæ¶ˆæ¯ 

history.messages è·å–æ‰€æœ‰æ¶ˆæ¯ 

clearæ¸…ç©ºæ‰€æœ‰æ¶ˆæ¯

```python
#æ¶ˆæ¯å†å²ç»„ä»¶ChatMessageHistoryçš„ä½¿ç”¨

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder, 
    HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.chat_message_histories import ChatMessageHistory
from models import get_lc_model_client, get_ali_model_client



chat_template = ChatPromptTemplate.from_messages(
    [

        SystemMessagePromptTemplate.from_template("ä½ æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹"),
        HumanMessagePromptTemplate.from_template("{input}"),
        # ä½œç”¨å°±æ˜¯å‘æç¤ºè¯ä¸­æ’å…¥ä¸€æ®µä¸Šä¸‹æ–‡æ¶ˆæ¯
        # ("placeholder", "{messages}"),
        MessagesPlaceholder(variable_name="messages"),

    ]
)

client = get_ali_model_client()

parser = StrOutputParser()
chain =  chat_template | client | parser

#  åˆ›å»ºæ¶ˆæ¯å†å²è®°å½•
chat_history = ChatMessageHistory()

while True:
    user_input = input("ç”¨æˆ·ï¼š")
    if user_input == "exit":
        break
    # æ·»åŠ ç”¨æˆ·è¾“å…¥
    chat_history.add_user_message(user_input)
    # è®¿é—®LLMæ—¶ï¼Œchat_history.messages è·å–æ‰€æœ‰çš„å†å²æ¶ˆæ¯
    response = chain.invoke({'messages': chat_history.messages, 'input': user_input})
    print("chat_history:",chat_history.messages)
    print(f"å¤§æ¨¡å‹å›å¤ã€‹ã€‹ã€‹ï¼š{response}")
    # å°†å¤§æ¨¡å‹çš„å›å¤åŠ å…¥å†å²è®°å½•
    chat_history.add_ai_message(response)

# #ç¬¬ä¸€è½®å¯¹è¯ï¼š æ·»åŠ ç”¨æˆ·çš„æé—®æ¶ˆæ¯
# chat_history.add_user_message('ä½ å¥½ï¼Œæˆ‘æ˜¯å¤§ç™½')#ç”¨æˆ·æé—®åŠ å…¥å†å²è®°å½•
# response = chain.invoke({'messages': chat_history.messages})
#
# print(response)
# #ç¬¬ä¸€è½®å¯¹è¯ï¼š æ·»åŠ æ¨¡å‹åº”ç­”æ¶ˆæ¯
# chat_history.add_ai_message(response) #æ¨¡å‹åº”ç­”åŠ å…¥å†å²è®°å½•
# print("chat_history:",chat_history.messages)
#
# # ç¬¬äºŒè½®å¯¹è¯ï¼š æ·»åŠ ç”¨æˆ·çš„æé—®æ¶ˆæ¯
# chat_history.add_user_message('ä½ å¥½ï¼Œæˆ‘æ˜¯è°ï¼Ÿ')#ç”¨æˆ·æé—®åŠ å…¥å†å²è®°å½•
# print(chain.invoke({'messages': chat_history.messages}))
```

#### RedisChatMessageHistory

å°†è®°å¿†å­˜å‚¨åœ¨å†…å­˜ï¼Œç³»ç»Ÿé‡å¯æˆ–è€…ç”µè„‘å…³æœºå°†æ¸…ç©ºå†…å­˜ï¼Œè¿™ä¸ªæ—¶å€™è®°å¿†ä¹Ÿå°±è‡ªåŠ¨æ¸…é™¤äº†ï¼Œèƒ½ä¸èƒ½å®ç°é•¿æœŸä¿å­˜ï¼Ÿ

ç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œå¯ä»¥ä½¿ç”¨å…³ç³»å‹æ•°æ®åº“ã€ESã€æ¶ˆæ¯é˜Ÿåˆ—ç­‰ä¿å­˜ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å†…å­˜æ•°æ®åº“ä¿å­˜ï¼Œredisæ•°æ®åº“æ˜¯æ¯”è¾ƒå¸¸ç”¨çš„ä¸€ç§ã€‚

```python
# 1.pip install -qU langchain-redis langchain-openai redis
from gradio.themes.builder_app import history
from langchain_redis import RedisChatMessageHistory

from models import get_lc_model_client
# session_id è¯†åˆ«ç”¨æˆ· redis_url è®¿é—®è·¯å¾„
history = RedisChatMessageHistory(session_id="my_session_id", redis_url="redis://localhost:6379")


# history.clear()  æ¸…ç©ºå†å²æ¶ˆæ¯

client = get_lc_model_client()
# history.add_user_message("ä½ æ˜¯è°ï¼Ÿ")
#
# aimessage = client.invoke(history.messages)
# history.add_ai_message(aimessage)
# print(aimessage)

history.add_user_message("é‡å¤ä¸€æ¬¡")
print(history.messages)
aimessage = client.invoke(history.messages)
history.add_ai_message(aimessage)
print(aimessage)

# RedisChatMessageHistory å’Œ ChatMessageHistory éƒ½æœ‰ç›¸åŒçš„API
```

#### RunnableWithMessageHistory

`RunnableWithMessageHistory` æ˜¯ **LangChain/LangGraph** æ¡†æ¶ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œä¸“é—¨ç”¨äºä¸ºé“¾å¼æ“ä½œï¼ˆChainï¼‰æˆ–å¯è¿è¡Œå¯¹è±¡ï¼ˆRunnableï¼‰**æ·»åŠ å¯¹è¯å†å²ç®¡ç†èƒ½åŠ›**ã€‚å®ƒä½¿å¾—æ„å»ºèƒ½å¤Ÿè®°ä½å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡çš„èŠå¤©æœºå™¨äººæˆ–å¯¹è¯ç³»ç»Ÿå˜å¾—éå¸¸ç®€å•ã€‚å®ƒèƒ½**åœ¨å¤šæ¬¡è°ƒç”¨ä¹‹é—´è‡ªåŠ¨ç»´æŠ¤å’Œæ³¨å…¥å¯¹è¯å†å²è®°å½•**ï¼Œè®©LLMèƒ½å¤ŸåŸºäºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡ç”Ÿæˆå›å¤ã€‚

```python
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

#é¦–å…ˆåˆ›å»ºä¸€ä¸ªåŸºç¡€çš„Runnableï¼ˆæ¯”å¦‚ä¸€ä¸ªç®€å•çš„é“¾ï¼‰
client = get_lc_model_client()

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])
chain = prompt | client

#2. åˆ›å»ºRunnableWithMessageHistory

chain_with_history = RunnableWithMessageHistory(
    chain,  # åŸºç¡€çš„é“¾
    get_session_history,  # è·å–å†å²è®°å½•çš„å‡½æ•°
    input_messages_key="input",  # è¾“å…¥æ¶ˆæ¯çš„é”®
    history_messages_key="history"  # å†å²æ¶ˆæ¯çš„é”®
)

#3. å®šä¹‰è·å–å†å²è®°å½•çš„å‡½æ•°

store = {}  # ç®€å•çš„å†…å­˜å­˜å‚¨

def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

#4. ä½¿ç”¨ï¼ˆå¿…é¡»æŒ‡å®šsession_idæ¥åŒºåˆ†ä¸åŒå¯¹è¯ï¼‰

response = chain_with_history.invoke(
    {"input": "ä½ å¥½ï¼Œæˆ‘å«å°æ˜"},
    config={"configurable": {"session_id": "user123"}}
)
```



## å…­ã€RAGå®ç°

åœ¨è®¸å¤šLLMåº”ç”¨ç¨‹åºä¸­ï¼Œç”¨æˆ·ç‰¹å®šçš„æ•°æ®ä¸åœ¨å¤§æ¨¡å‹ä¸­ï¼Œå¯èƒ½åœ¨å¤–éƒ¨ç³»ç»Ÿæˆ–æ–‡æ¡£ä¸­ã€‚å¦‚ä½•ä½¿ç”¨è¿™äº›å¤–éƒ¨æ•°æ®æ¥å¢å¼ºå‘¢ï¼Ÿ

â€‹        LangChainä¸­çš„æ•°æ®è¿æ¥ç»„ä»¶åŒ…æ‹¬å‡ ä¸ªå…³é”®æ¨¡å—ï¼šæ–‡æ¡£åŠ è½½å™¨ ã€æ–‡æ¡£åˆ‡åˆ†ã€æ–‡æœ¬åµŒå…¥ã€å‘é‡å­˜å‚¨ã€æ£€ç´¢å™¨ ç­‰ç­‰ã€‚

  

### 1.æ–‡æ¡£ä¸æ–‡æ¡£åŠ è½½ä¸åˆ‡å‰²

æ–‡æ¡£ç±»å‹æœ‰å¾ˆå¤šï¼Œå¯ä»¥æ˜¯ç®€å•çš„æ–‡æœ¬æ–‡ä»¶ã€Wordæ–‡æ¡£ã€Excelè¡¨æ ¼ã€Pdfæ–‡ä»¶ç­‰ç­‰ï¼Œç”šè‡³æ˜¯ å„ç§è§†é¢‘çš„è½¬å½•æ–‡ä»¶

åŠ è½½å™¨æœ‰ loadæ–¹æ³•ï¼Œç”¨äºä»æŒ‡å®šçš„æ•°æ®æºè¯»å–æ•°æ®ï¼Œå¹¶å°†å…¶è½¬æ¢æˆä¸€ä¸ªæˆ–å¤šä¸ªæ–‡æ¡£ã€‚è¿™ä½¿å¾— LangChain èƒ½å¤Ÿå¤„ç†å„ç§å½¢å¼çš„è¾“å…¥æ•°æ®ï¼Œä¸ä»…ä»…é™äºæ–‡æœ¬æ•°æ®ï¼Œè¿˜å¯ä»¥æ˜¯ç½‘é¡µã€è§†é¢‘å­—å¹•ç­‰ã€‚

LangChainæœ‰å¾ˆå¼ºçš„æ•°æ®åŠ è½½èƒ½åŠ›ï¼Œæä¾›äº†å¾ˆå¤šå¸¸è§çš„æ•°æ®æ ¼å¼çš„æ”¯æŒï¼Œä¾‹å¦‚CSVã€æ–‡ä»¶ç›®å½•ã€HTMLã€JSONã€MarkdownåŠPDFç­‰ã€‚

TXTæ–‡æ¡£:TextLoader
PDFæ–‡æ¡£:PyPDFLoader
CSVæ–‡æ¡£:CSVLoader
JSONæ–‡æ¡£:JSONLoader
HTMLæ–‡æ¡£:UnstructuredHTMLLoader
MDæ–‡æ¡£:UnStructuredMarkdownLoader
æ–‡ä»¶ç›®å½•:DirectoryLoader

åœ¨å‰é¢çš„RAGåŸºç¡€ç« èŠ‚ï¼Œæˆ‘ä»¬å·²ç»äº†è§£äº†æ–‡æœ¬çš„åˆ†å‰²ã€‚LangChainä¸ºæ­¤ä¹Ÿæä¾›äº†å¾ˆå¤šç°æˆçš„åˆ†å‰²å™¨ï¼š
CharacterTextSplitterï¼š   åŸºäºå­—ç¬¦(é»˜è®¤ä¸º"\n\n")è¿›è¡Œåˆ‡å‰²ï¼Œå¹¶é€šè¿‡å­—ç¬¦æ•°é‡æ¥æµ‹é‡æ–‡æœ¬å—çš„å¤§å°ã€‚ä½¿ç”¨chunk_sizeå±æ€§å¯è®¾ç½®æ–‡æœ¬å—çš„å¤§å°ï¼Œä½¿ç”¨chunk_overlap å±æ€§è®¾ç½®æ–‡æœ¬å—ä¹‹é—´çš„æœ€å¤§é‡å ã€‚
RecursiveCharacterTextSplitterï¼š  åœ¨å‰é¢æˆ‘ä»¬å·²ç»äº†è§£è¿‡ï¼Œé™¤äº†è‡ªç„¶è¯­è¨€ï¼Œå®ƒä¹Ÿæ”¯æŒå¯¹ç‰¹å®šçš„ç¼–ç¨‹è¯­è¨€(â€˜JavaScriptâ€™ã€â€˜cppâ€™ã€â€˜goâ€™ã€â€˜javaâ€™ ã€â€˜phpâ€™ã€â€˜pythonâ€™ç­‰ç­‰)çš„ä»£ç è¿›è¡Œåˆ‡å‰²ã€‚
MarkdownHeaderTextSplitterï¼šå¯ä»¥æ ¹æ®æŒ‡å®šçš„ä¸€ç»„æ ‡é¢˜æ¥åˆ‡å‰²ä¸€ä¸ªMarkdown æ–‡æ¡£ã€‚
â€¦â€¦â€¦.
å†…ç½®åˆ†å‰²å™¨ï¼šhttps://python.langchain.com/api_reference/text_splitters/index.html

```python
# 1.æŒ‡å®šè¦åŠ è½½çš„Wordæ–‡æ¡£è·¯å¾„
loader = Docx2txtLoader("äººäº‹ç®¡ç†æµç¨‹.docx")

# åŠ è½½æ–‡æ¡£ã€è½¬æ¢æ ¼å¼åŒ–æˆdocument
documents = loader.load()
# print(len(documents))
# # Document(metadata={'source': 'äººäº‹ç®¡ç†æµç¨‹.docx'}, page_content='é›†å›¢ç®¡ç†åˆ¶åº¦\n\näºº')
# print(documents)

# æ–‡æ¡£åˆ‡å‰² é€’å½’åˆ‡å‰²
# separators
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,

    # separators=[åˆ†éš”ç¬¦]
)
# é€šè¿‡åˆ†å‰²å™¨è·å–document :create_documents   split_documents  ä¼ å…¥ä¸€ä¸ªdocumentå¯¹è±¡ï¼Œè¿”å›ä¸€ä¸ªdocumentå¯¹è±¡åˆ—è¡¨
split_documents = text_splitter.split_documents(documents)
```

### 2.æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆEmbedding Modelï¼‰

LangChain æ¡†æ¶æä¾›äº†ä¸€ä¸ªåä¸ºEmbeddings çš„ç±»ï¼Œå®ƒä¸ºå¤šç§æ–‡æœ¬åµŒå…¥æ¨¡å‹(å¦‚OpenAIã€Cohcreã€HuggingFace ç­‰)æä¾›äº†ç»Ÿä¸€çš„æ¥å£ã€‚é€šè¿‡è¯¥ç±»å®ä¾‹åŒ–çš„åµŒäººæ¨¡å‹åŒ…è£…å™¨ï¼Œå¯ä»¥å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡æ•°æ®ï¼ŒåŒæ—¶å°†æœç´¢çš„é—®é¢˜ä¹Ÿè½¬æ¢ä¸ºå‘é‡æ•°æ®ï¼Œè¿™ä½¿å¾—å¯é€šè¿‡è®¡ç®—æœç´¢é—®é¢˜å’Œæ–‡æ¡£åœ¨å‘é‡ç©ºé—´ä¸­çš„è·ç¦»ï¼Œæ¥å¯»æ‰¾åœ¨å‘é‡ç©ºé—´ä¸­æœ€ç›¸ä¼¼çš„æ–‡æœ¬ã€‚

```python
# æ¨¡å‹åŒ…è£…å™¨ï¼šå¤§æ¨¡å‹åˆ†æˆä¸‰ç±»ï¼šLLM  èŠå¤©æ¨¡å‹  åµŒå…¥æ¨¡å‹
# è·å¾—ä¸€ä¸ªé˜¿é‡Œé€šä¹‰åƒé—®åµŒå…¥æ¨¡å‹çš„å®ä¾‹ï¼ŒåŒæ ·åœ¨models.pyä¸­è¢«åŒ…è£…ä¸ºget_ali_embeddings()
from langchain_community.embeddings import DashScopeEmbeddings
# åµŒå…¥æ¨¡å‹çš„æ¨¡å‹åŒ…è£…å™¨
llm_embeddings = DashScopeEmbeddings(
    # æ¨¡å‹åç§°
    model=ALI_TONGYI_EMBEDDING_MODEL,
    # API_KEY
    dashscope_api_key=os.getenv(ALI_TONGYI_API_KEY_OS_VAR_NAME)
)
```





### 3.å‘é‡æ•°æ®åº“ï¼š

```python
# å®ä¾‹åŒ–å‘é‡ç©ºé—´ï¼Œå‘é‡åŒ–+å‘é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­
vector_store = Chroma.from_documents(documents=split_documents,embedding=llm_embeddings)

#å±•ç¤ºç›¸ä¼¼åº¦æŸ¥è¯¢ï¼Œå®é™…ä¸šåŠ¡ä¸­å¯ä»¥ä¸è¦
# print(vector_store.similarity_search("ç‹¸èŠ±çŒ«"))
# print("--"*15)
# #æŒ‰ç›¸ä¼¼åº¦çš„åˆ†æ•°è¿›è¡Œæ’åºï¼Œåˆ†æ•°å€¼è¶Šå°ï¼Œè¶Šç›¸ä¼¼ï¼ˆå…¶å®æ˜¯L2è·ç¦»ï¼‰
# print(vector_store.similarity_search_with_score("ç‹¸èŠ±çŒ«"))
```



### 4.æ£€ç´¢å™¨

æ£€ç´¢å™¨ (Retriever) æ˜¯ RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä»çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚
1.åˆ›å»º FAISS æ£€ç´¢å™¨
retriever_faiss = FAISS.from_texts(texts, embeddings).as_retriever()
2.åˆ›å»º Chroma æ£€ç´¢å™¨
retriever_chroma = Chroma.from_documents(docs, embeddings).as_retriever()
3.å…³é”®è¯æ£€ç´¢å™¨
retriever_bm25 = BM25Retriever.from_texts(texts)
4.æ··åˆæ£€ç´¢å™¨ (ç»“åˆè¯­ä¹‰+å…³é”®è¯)
ensemble_retriever = EnsembleRetriever(
    retrievers=[retriever_faiss, retriever_bm25],
    weights=[0.7, 0.3]  # æƒé‡åˆ†é…
)

æ£€ç´¢å™¨åº•å±‚é»˜è®¤ä½¿ç”¨çš„æ˜¯è¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œæ£€ç´¢ï¼Œå¯ä»¥é€šè¿‡allowed_search_types å‚æ•°è®¾ç½®æ£€ç´¢æ–¹å¼

```python
VectorStoreRetriever ä¸­allowed_search_types å‚æ•°è®¾ç½®æ£€ç´¢æ–¹å¼ï¼š
 similarity  similarity_score_threshold  mmr æœ€å¤§è¾¹é™…ç›¸ä¼¼æ€§æ£€ç´¢:
 mmr æœ€å¤§è¾¹é™…ç›¸ä¼¼æ€§æ£€ç´¢: 1.ç›¸ä¼¼æ€§æ£€ç´¢ï¼›2.æ ¹æ®ç›¸ä¼¼æ€§å¾—åˆ†è¿›è¡Œè¿‡æ»¤+ä¸å·²é€‰æ‹©ç»“æœè¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…  ï¼ˆå»é™¤å†—ä½™ï¼Œé¼“åŠ±å¤šæ ·æ€§ï¼‰
```



```python
# å®ä¾‹åŒ–å‘é‡ç©ºé—´ï¼Œå‘é‡åŒ–+å‘é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­
vector_store = Chroma.from_documents(documents=split_documents,embedding=llm_embeddings)

#å±•ç¤ºç›¸ä¼¼åº¦æŸ¥è¯¢ï¼Œå®é™…ä¸šåŠ¡ä¸­å¯ä»¥ä¸è¦
# print(vector_store.similarity_search("ç‹¸èŠ±çŒ«"))
# print("--"*15)
# #æŒ‰ç›¸ä¼¼åº¦çš„åˆ†æ•°è¿›è¡Œæ’åºï¼Œåˆ†æ•°å€¼è¶Šå°ï¼Œè¶Šç›¸ä¼¼ï¼ˆå…¶å®æ˜¯L2è·ç¦»ï¼‰
# print(vector_store.similarity_search_with_score("ç‹¸èŠ±çŒ«"))


# æ£€ç´¢å™¨å¯¹è±¡ï¼šæ£€ç´¢æ–‡æ¡£ï¼Œå¯ä»¥æ ¹æ®éœ€è¦å¯¹æ£€ç´¢åçš„ç»“æœåšå„ç§å¤„ç†
# VectorStoreRetriever ä¸­allowed_search_types å‚æ•°è®¾ç½®æ£€ç´¢æ–¹å¼ï¼š
# similarity  similarity_score_threshold  mmr æœ€å¤§è¾¹é™…ç›¸ä¼¼æ€§æ£€ç´¢:
# mmr æœ€å¤§è¾¹é™…ç›¸ä¼¼æ€§æ£€ç´¢: 1.ç›¸ä¼¼æ€§æ£€ç´¢ï¼›2.æ ¹æ®ç›¸ä¼¼æ€§å¾—åˆ†è¿›è¡Œè¿‡æ»¤+ä¸å·²é€‰æ‹©ç»“æœè¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…  ï¼ˆå»é™¤å†—ä½™ï¼Œé¼“åŠ±å¤šæ ·æ€§ï¼‰
retriever = vector_store.as_retriever()
# result = retriever.invoke("æ™‹å‡")
```



å®Œæ•´çš„Ragæ¡ˆä¾‹ï¼š

```python
import os
# å®‰è£… pip install langchain_chroma
# åŠ è½½wordæ–‡æ¡£ å®‰è£… pip install docx2txt
# åŠ è½½jsonæ–‡æ¡£ å®‰è£… pip install jq
# åŠ è½½pdfæ–‡æ¡£  å®‰è£… pip install pymupdf
# åŠ è½½HTMLæ–‡æ¡£ å®‰è£… pip install unstructured
# åŠ è½½MDæ–‡æ¡£   å®‰è£… pip install markdown +  pip install unstructured
import langchain
# pip install langchain-chroma
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.documents import Document
from langchain_core.vectorstores import VectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter

from models import get_lc_model_client, ALI_TONGYI_API_KEY_OS_VAR_NAME, ALI_TONGYI_EMBEDDING_MODEL, get_ali_model_client


#è·å¾—è®¿é—®å¤§æ¨¡å‹å®¢æˆ·ç«¯
client = get_ali_model_client()

#ç›´æ¥äº†è§£LangChainä¸­çš„â€œæ–‡æ¡£â€(Document)çš„å…·ä½“å†…å®¹ï¼Œè¿™é‡Œæˆ‘ä»¬è·³è¿‡äº†æ–‡æ¡£ä¸æ–‡æ¡£åŠ è½½ï¼Œæ–‡æ¡£åˆ‡å‰²å’Œæ–‡æ¡£è½¬æ¢è¿‡ç¨‹
#æ–‡æ¡£çš„æ¨¡æ‹Ÿæ•°æ®
# documents = [
#     Document(
#         page_content="çŒ«æ˜¯æŸ”è½¯å¯çˆ±çš„åŠ¨ç‰©ï¼Œä½†ç›¸å¯¹ç‹¬ç«‹",
#         metadata={"source": "å¸¸è§åŠ¨ç‰©å® ç‰©æ–‡æ¡£"},
#     ),
#     Document(
#         page_content="ç‹—æ˜¯äººç±»å¾ˆæ—©å¼€å§‹çš„åŠ¨ç‰©ä¼´ä¾£ï¼Œå…·æœ‰å›¢é˜Ÿèƒ½åŠ›",
#         metadata={"source": "å¸¸è§åŠ¨ç‰©å® ç‰©æ–‡æ¡£"},
#     ),
#     Document(
#         page_content="é‡‘é±¼æ˜¯æˆ‘ä»¬å¸¸å¸¸å–‚å…»çš„è§‚èµåŠ¨ç‰©ä¹‹ä¸€ï¼Œæ´»æ³¼çµåŠ¨",
#         metadata={"source": "é±¼ç±»å® ç‰©æ–‡æ¡£"},
#     ),
#     Document(
#         page_content="é¹¦é¹‰æ˜¯çŒ›ç¦½ï¼Œä½†èƒ½å¤Ÿæ¨¡ä»¿äººç±»çš„è¯­è¨€",
#         metadata={"source": "é£ç¦½å® ç‰©æ–‡æ¡£"},
#     ),
#     Document(
#         page_content="å…”å­æ˜¯å°æœ‹å‹æ¯”è¾ƒå–œæ¬¢çš„å® ç‰©ï¼Œä½†æ˜¯æ¯”è¾ƒéš¾å–‚å…»",
#         metadata={"source": "å¸¸è§åŠ¨ç‰©å® ç‰©æ–‡æ¡£"},
#     ),
# ]

from langchain_community.document_loaders import UnstructuredWordDocumentLoader, Docx2txtLoader

# 1.æŒ‡å®šè¦åŠ è½½çš„Wordæ–‡æ¡£è·¯å¾„
loader = Docx2txtLoader("äººäº‹ç®¡ç†æµç¨‹.docx")

# åŠ è½½æ–‡æ¡£ã€è½¬æ¢æ ¼å¼åŒ–æˆdocument
documents = loader.load()
# print(len(documents))
# # Document(metadata={'source': 'äººäº‹ç®¡ç†æµç¨‹.docx'}, page_content='é›†å›¢ç®¡ç†åˆ¶åº¦\n\näºº')
# print(documents)

# æ–‡æ¡£åˆ‡å‰² é€’å½’åˆ‡å‰² separators
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
)
# é€šè¿‡åˆ†å‰²å™¨è·å–document :create_documents   split_documents  ä¼ å…¥ä¸€ä¸ªdocumentå¯¹è±¡ï¼Œè¿”å›ä¸€ä¸ªdocumentå¯¹è±¡åˆ—è¡¨
split_documents = text_splitter.split_documents(documents)

# æ¨¡å‹åŒ…è£…å™¨ï¼šå¤§æ¨¡å‹åˆ†æˆä¸‰ç±»ï¼šLLM  èŠå¤©æ¨¡å‹  åµŒå…¥æ¨¡å‹
# è·å¾—ä¸€ä¸ªé˜¿é‡Œé€šä¹‰åƒé—®åµŒå…¥æ¨¡å‹çš„å®ä¾‹ï¼ŒåŒæ ·åœ¨models.pyä¸­è¢«åŒ…è£…ä¸ºget_ali_embeddings()
from langchain_community.embeddings import DashScopeEmbeddings
# åµŒå…¥æ¨¡å‹çš„æ¨¡å‹åŒ…è£…å™¨
llm_embeddings = DashScopeEmbeddings(
    # æ¨¡å‹åç§°
    model=ALI_TONGYI_EMBEDDING_MODEL,
    # API_KEY
    dashscope_api_key=os.getenv(ALI_TONGYI_API_KEY_OS_VAR_NAME)
)

# å®ä¾‹åŒ–å‘é‡ç©ºé—´ï¼Œå‘é‡åŒ–+å‘é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­
vector_store = Chroma.from_documents(documents=split_documents,embedding=llm_embeddings)

retriever = vector_store.as_retriever()

message = """ 
ä»…ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š
{question}
ä¸Šä¸‹æ–‡ï¼š
{context}
"""
prompt_template = ChatPromptTemplate.from_messages([('human',message)])
# ç”¨RunnablePassthroughå…è®¸æˆ‘ä»¬å°†ç”¨æˆ·çš„å…·ä½“é—®é¢˜åœ¨å®é™…ä½¿ç”¨è¿‡ç¨‹ä¸­è¿›è¡ŒåŠ¨æ€ä¼ å…¥
chain = {"question":RunnablePassthrough(),"context":retriever} | prompt_template | client

#ç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
resp = chain.invoke("æ™‹å‡")
print(resp.content)
```

## ä¸ƒã€LangChainå·¥å…·çš„è°ƒç”¨

å¦‚æœå¤§æ¨¡å‹åªæ˜¯å’Œæˆ‘ä»¬è¿›è¡Œè‡ªç„¶è¯­è¨€èŠå¤©ï¼Œå®ƒå›ºç„¶æœ‰ç”¨ï¼Œä½†æ˜¯å†³å®šæ²¡æœ‰ç°åœ¨è¿™æ ·ç”¨é€”å¹¿æ³›ã€‚æˆ‘ä»¬å¸Œæœ›å¤§æ¨¡å‹èƒ½åšæ›´å¤šçš„äº‹ï¼Œæ¯”å¦‚åƒäººç±»ä¸€æ ·ä½¿ç”¨å·¥å…·

ç°åœ¨æ¯”è¾ƒç«çˆ†çš„MCPã€Agent2Agentè¯´åˆ°åº•ï¼Œéƒ½ç¦»ä¸å¼€å¤§æ¨¡å‹å¯¹å·¥å…·çš„è°ƒç”¨ï¼Œé‚£ä¹ˆå¤§æ¨¡å‹æ˜¯å¦‚ä½•è¿›è¡Œå·¥å…·è°ƒç”¨çš„å‘¢ï¼Ÿé¦–å…ˆéœ€è¦è®©å¤§æ¨¡å‹çŸ¥é“æœ‰å“ªäº›å¯ç”¨çš„å·¥å…·ï¼Œç„¶åæ‰èƒ½è¿›è¡Œåˆ†æåˆ¤æ–­ä»€ä¹ˆéœ€æ±‚å¯ä»¥è°ƒç”¨å“ªäº›å·¥å…·ï¼Œæœ€åæ‰æ˜¯çœŸæ­£çš„è°ƒç”¨å·¥å…·ï¼Œå°†å·¥å…·çš„è¿”å›ç»™åˆ°å¤§æ¨¡å‹ï¼Œä»è€Œç”Ÿæˆç»“æœè¿”å›ã€‚

### 1.å·¥å…·Toolsçš„å®šä¹‰

å·¥å…·æ˜¯ä»£ç†ã€é“¾æˆ–LLMå¯ä»¥ç”¨æ¥ä¸ä¸–ç•Œäº’åŠ¨çš„æ¥å£ã€‚å®ƒä»¬ç»“åˆäº†å‡ ä¸ªè¦ç´ 

- å·¥å…·çš„åç§°
- å·¥å…·çš„æè¿°
- è¯¥å·¥å…·è¾“å…¥çš„JSONæ¨¡å¼
- è¦è°ƒç”¨çš„å‡½æ•°
- æ˜¯å¦åº”å°†å·¥å…·ç»“æœç›´æ¥è¿”å›ç»™ç”¨æˆ·

LangChainé€šè¿‡æä¾›ç»Ÿä¸€æ¡†æ¶é›†æˆåŠŸèƒ½çš„å…·ä½“å®ç°ï¼Œåœ¨æ¡†æ¶å†…ï¼Œæ¯ä¸ªåŠŸèƒ½è¢«å°è£…æˆä¸€ä¸ªå·¥å…·ã€‚Tools ç»„ä»¶ä¸­å¯ä»¥è°ƒç”¨çš„å„ç§å·¥å…·ç±»ã€‚LangChain æä¾›äº†ä¸€ç»„é¢„å®šä¹‰çš„Tools ç»„ä»¶ï¼ŒåŒæ—¶ä¹Ÿå…è®¸ç”¨æˆ·è‡ªå®šä¹‰Toolsç»„ä»¶ä»¥æ»¡è¶³ç‰¹å®šçš„éœ€æ±‚ã€‚

Toolkits ç»„ä»¶æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ç»„ä»¶é›†åˆï¼Œå®ƒåŒ…æ‹¬äº†å¤šä¸ªç”¨äºå®ç°ç‰¹å®šç›®æ ‡çš„ Toolsç»„ä»¶ï¼ŒLangChain ä¹Ÿæä¾›äº†é¢„å®šä¹‰çš„ Toolkits ç»„ä»¶ã€‚

å·¥å…·å®šä¹‰ï¼š

```python
@tool
def get_date():
    """ è·å–åŒ—äº¬ä»Šå¤©çš„å¤©æ°” """
    return datetime.date.today().strftime("%Y-%m-%d")

# schema è‹±è¯­ï¼Œè¯­æ³•ï¼šä½œç”¨å®šä¹‰è‹±è¯­è¯­å¥æ”¹æ€ä¹ˆè¯´æ€ä¹ˆå†™   xml

import webbrowser
@tool
def open_browser(url, browser_name=None):
    """ è·å–æµè§ˆå™¨ï¼Œæ‰“å¼€ç½‘ç«™ """
    if browser_name:
        # è·å–ç‰¹å®šæµè§ˆå™¨çš„æ§åˆ¶å™¨
        browser = webbrowser.get(browser_name)
    else:
        # ä½¿ç”¨é»˜è®¤æµè§ˆå™¨
        browser = webbrowser
    # æ‰“å¼€æµè§ˆå™¨å¹¶å¯¼èˆªåˆ°æŒ‡å®šçš„URL
    browser.open(url)
```

å¤§æ¨¡å‹ç»‘å®šå·¥å…·ï¼š

```python
# api_key  ç§˜é’¥  modelï¼šæ¨¡å‹åç§° base_urlï¼šæ¨¡å‹è¿æ¥çš„urlåœ°å€
model = ChatOpenAI(api_key=os.getenv("DASHSCOPE_API_KEY"), model="qwen-max", base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

# resp = model.invoke("ä»Šå¤©æ˜¯å‡ æœˆå‡ å·ï¼Ÿ")
# print(resp)

@tool
def get_date():
    """ è·å–åŒ—äº¬ä»Šå¤©çš„å¤©æ°” """
    return datetime.date.today().strftime("%Y-%m-%d")

# schema è‹±è¯­ï¼Œè¯­æ³•ï¼šä½œç”¨å®šä¹‰è‹±è¯­è¯­å¥æ”¹æ€ä¹ˆè¯´æ€ä¹ˆå†™   xml

import webbrowser
@tool
def open_browser(url, browser_name=None):
    """ è·å–æµè§ˆå™¨ï¼Œæ‰“å¼€ç½‘ç«™ """
    if browser_name:
        # è·å–ç‰¹å®šæµè§ˆå™¨çš„æ§åˆ¶å™¨
        browser = webbrowser.get(browser_name)
    else:
        # ä½¿ç”¨é»˜è®¤æµè§ˆå™¨
        browser = webbrowser
    # æ‰“å¼€æµè§ˆå™¨å¹¶å¯¼èˆªåˆ°æŒ‡å®šçš„URL
    browser.open(url)

# å¤§æ¨¡å‹å®¢æˆ·ç«¯ç»‘å®šå·¥å…·
tool_llm = model.bind_tools([get_date, open_browser])
```



### 2.Langchainå·¥å…·çš„è°ƒç”¨

é€šè¿‡Langchain1.0ä¸­çš„create_agentæ¥å®ç°å·¥å…·è°ƒç”¨ï¼Œå…¶ä¸­å‚æ•°toolså°±æ˜¯ç”¨æ¥ç»‘å®šå·¥å…·åˆ—è¡¨

```python
import datetime
import os

from dashscope import api_key
from langchain.agents import create_agent

from langchain_openai import ChatOpenAI
from langchain.chat_models import  init_chat_model
from langchain_core.prompts import  PromptTemplate
# from langchain_core.tools import tool
from langchain.tools import tool


# api_key  ç§˜é’¥  modelï¼šæ¨¡å‹åç§° base_urlï¼šæ¨¡å‹è¿æ¥çš„urlåœ°å€
model = ChatOpenAI(api_key=os.getenv("DASHSCOPE_API_KEY"), model="qwen-max", base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

# æ³¨æ„ï¼šå‡½æ•°çš„æè¿°å¿…é¡»å†™åœ¨å‡½æ•°ä½“ä¸­çš„ç¬¬ä¸€è¡Œ
@tool
def get_date():
    """ è·å–ä»Šå¤©çš„å…·ä½“æ—¥æœŸ """
    # """ è·å–ä»Šå¤©çš„åŒ—äº¬çš„å¤©æ°” """
    return datetime.date.today().strftime("%Y-%m-%d")


import webbrowser
@tool
def open_browser(url, browser_name=None):
    """ è·å–æµè§ˆå™¨ï¼Œæ‰“å¼€ç½‘ç«™ """
    if browser_name:
        # è·å–ç‰¹å®šæµè§ˆå™¨çš„æ§åˆ¶å™¨
        browser = webbrowser.get(browser_name)
    else:
        # ä½¿ç”¨é»˜è®¤æµè§ˆå™¨
        browser = webbrowser
    # æ‰“å¼€æµè§ˆå™¨å¹¶å¯¼èˆªåˆ°æŒ‡å®šçš„URL
    browser.open(url)

# å¤§æ¨¡å‹å®¢æˆ·ç«¯ç»‘å®šå·¥å…·
agent = create_agent(
    model,
    tools=[get_date, open_browser],
)
# æ‰§è¡Œagent
# result = agent.invoke({"messages":[{"role":"user","content":"å¸®æˆ‘æ‰“å¼€æ·˜å®"}]})
# result = agent.invoke({"messages":[{"role":"user","content":"ä»Šå¤©æ˜¯å‡ æœˆå‡ å·ï¼Ÿ"}]})
# è·å–åŒ—äº¬çš„ä»Šå¤©çš„å¤©æ°”
result = agent.invoke({"messages":[{"role":"user","content":"å¸®æˆ‘æ‰“å¼€æ·˜å®"}]})
print( result)
```



### 3.Langchainä¸­é¢„å®šä¹‰å·¥å…·çš„ä½¿ç”¨

Langchainä¸­å®šä¹‰äº†å¾ˆå¤šå·¥å…·ï¼Œæˆ‘ä»¬ç°åœ¨ä»¥arxivå·¥å…·ä¸ºä¾‹ç»™å¤§å®¶æ¼”ç¤ºï¼Œarxivå·¥å…·æ˜¯ç”¨æ¥åœ¨arxiv.orgç½‘ç«™æŸ¥è¯¢æ£€ç´¢è®ºæ–‡çš„ï¼Œé¢„å®šä¹‰çš„å·¥å…·Langchainå·²ç»å®šä¹‰å¥½äº†ï¼Œæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨load_tools()æ–¹æ³•å¯¼å…¥arxivå·¥å…·å°±è¡Œï¼Œå·¥å…·å…·ä½“çš„å®ç°ç±»æ˜¯ ArxivQueryRun(BaseTool)

```python
import os
import re

# éªŒè¯æ¨¡å‹è¿æ¥
from langchain_community.chat_models import ChatTongyi
from langchain_core.messages import HumanMessage

llm = ChatTongyi(api_key=os.environ.get("DASHSCOPE_API_KEY"))


# æ„å»ºä¸€ä¸ªåŸºäºarxivå·¥å…·çš„è®ºæ–‡æŸ¥è¯¢æ™ºèƒ½ä½“ï¼Œå®ç°æ ¹æ®è®ºæ–‡ç¼–å·æŸ¥è¯¢è®ºæ–‡ä¿¡æ¯çš„åŠŸèƒ½ã€‚
# ä½¿ç”¨load_tools()æ–¹æ³•å¯¼å…¥arxivå·¥å…·ï¼šclass ArxivQueryRun(BaseTool)
from langchain_community.agent_toolkits.load_tools import load_tools

# å¯¼å…¥arxivå·¥å…·
tools = load_tools(["arxiv"])

# çŸ­æœŸè®°å¿†æ„å»ºï¼šä½¿ç”¨InMemorySaver()å®ç°å•ä¼šè¯çš„çŸ­æœŸè®°å¿†
from langgraph.checkpoint.memory import InMemorySaver
# åˆ›å»ºçŸ­æœŸè®°å¿†å®ä¾‹
memory = InMemorySaver()
# ç³»ç»Ÿæç¤ºè¯è®¾è®¡ï¼šç®€æ´æ˜äº†åœ°å®šä¹‰Agentçš„è§’è‰²å’Œè¡Œä¸ºå‡†åˆ™
system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºæ–‡æŸ¥è¯¢åŠ©æ‰‹ï¼Œä½¿ç”¨arxivå·¥å…·ä¸ºç”¨æˆ·æŸ¥è¯¢è®ºæ–‡ä¿¡æ¯ï¼Œå›ç­”éœ€ç®€æ´å‡†ç¡®ï¼ŒåŒ…å«è®ºæ–‡æ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´å’Œæ ¸å¿ƒæ‘˜è¦ã€‚"
# ç»„è£…å¹¶è°ƒç”¨Agent
# ä½¿ç”¨create_agent()æ–¹æ³•ç»„è£…Agentï¼Œå¹¶é€šè¿‡invoke()æ–¹æ³•è°ƒç”¨ã€‚
from langchain.agents import create_agent

# ç»„è£…Agent
agent = create_agent(
    model=llm,
    tools=tools, #æ·»åŠ å·¥å…·åˆ—è¡¨ï¼Œç»‘å®šçš„è®ºæ–‡æŸ¥è¯¢çš„å·¥å…·
    system_prompt=system_prompt,
    checkpointer=memory  # ä¼ å…¥è®°å¿†ç»„ä»¶
)

# è°ƒç”¨AgentæŸ¥è¯¢è®ºæ–‡
result = agent.invoke(
    {"messages": [{"role": "user", "content": "è¯·æŸ¥è¯¢arxivè®ºæ–‡ç¼–å·1605.08386çš„ä¿¡æ¯"}]},
    # é…ç½®ä¼šè¯æ ‡è¯†ï¼Œç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·
    config={"configurable": {"thread_id": "user_1"}}  # ä¼šè¯å”¯ä¸€æ ‡è¯†ï¼Œç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·
)

# è¾“å‡ºç»“æœï¼ˆå–æœ€åä¸€æ¡æ¶ˆæ¯çš„å†…å®¹ï¼‰
print(result["messages"][-1].content)
"""
æ‰§è¡Œä¸Šè¿°ä»£ç åï¼ŒAgentä¼šè‡ªåŠ¨å®Œæˆä»¥ä¸‹æµç¨‹ï¼š

æ¥æ”¶ç”¨æˆ·æŸ¥è¯¢ï¼Œè¯†åˆ«éœ€è¦è°ƒç”¨arxivå·¥å…·
ç”Ÿæˆå·¥å…·è°ƒç”¨è¯·æ±‚ï¼Œä¼ å…¥è®ºæ–‡ç¼–å·1605.08386
æ‰§è¡Œarxivå·¥å…·ï¼Œè·å–è®ºæ–‡ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¶é—´ã€æ‘˜è¦ï¼‰
æ•´åˆå·¥å…·è¿”å›ç»“æœï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€å›å¤
"""
```



### 4.å…¶ä»–å·¥å…·è°ƒç”¨æ–¹å¼

langchainä¸­å¯ä»¥é€šè¿‡å†…ç½®é“¾çš„å½¢å¼è°ƒç”¨å·¥å…·ï¼Œä»¥ä¸‹æ˜¯è°ƒç”¨å·¥å…·æŸ¥è¯¢æ•°æ®åº“ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢mysqlæ•°æ®åº“ï¼š

```python
import os
from operator import itemgetter

import langchain
from langchain.chains.sql_database.query import create_sql_query_chain
from langchain_community.tools import QuerySQLDatabaseTool
from langchain_community.utilities import SQLDatabase

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
import re
from models import get_lc_model_client, get_ali_model_client, ALI_TONGYI_MAX_MODEL

#è·å¾—è®¿é—®å¤§æ¨¡å‹å®¢æˆ·ç«¯
client = get_ali_model_client(model=ALI_TONGYI_MAX_MODEL)
#æ•°æ®åº“é…ç½®
HOSTNAME ='127.0.0.1'
PORT ='3306'
DATABASE = 'world'
USERNAME = 'root'
PASSWORD ='1234'
MYSQL_URI ='mysql+mysqldb://{}:{}@{}:{}/{}?charset=utf8mb4'.format(USERNAME,PASSWORD,HOSTNAME,PORT,DATABASE)
db = SQLDatabase.from_uri(MYSQL_URI)

'''å¯¹äºé—®é¢˜ï¼š"è¯·ä»å›½å®¶è¡¨ä¸­æŸ¥è¯¢å‡ºChinaçš„æ‰€æœ‰æ•°æ®"ï¼Œè¦åˆ†ä¸ºå‡ æ­¥æ‰èƒ½å‡ºç»“æœï¼š
1ã€å¤§æ¨¡å‹åˆ¤æ–­è¿™ä¸ªé—®é¢˜éœ€è¦è°ƒç”¨å·¥å…·æŸ¥è¯¢æ•°æ®åº“ï¼Œè·å¾—æ‰€æœ‰çš„è¡¨åå’Œè¡¨ä¸­çš„å­—æ®µåï¼Œ
ç›®çš„æ˜¯çœ‹é‚£ä¸ªè¡¨æ‰æ˜¯å›½å®¶è¡¨ï¼Œå›½å®¶è¡¨æœ‰å“ªäº›å­—æ®µï¼Œ
2ã€å·¥å…·æ‰§è¡Œåï¼ŒæŠŠå·¥å…·æ‰§è¡Œç»“æœäº¤ç»™å¤§æ¨¡å‹
3ã€å¤§æ¨¡å‹æ ¹æ®å›½å®¶è¡¨å’Œå›½å®¶è¡¨ä¸­çš„å­—æ®µï¼Œç”ŸæˆSQLè¯­å¥
4ã€SQLè¯­å¥çš„æ‰§è¡Œä¾ç„¶éœ€è¦ä½¿ç”¨å·¥å…·
5ã€å·¥å…·æ‰§è¡Œåï¼ŒæŠŠå·¥å…·æ‰§è¡Œç»“æœäº¤ç»™å¤§æ¨¡å‹ï¼Œå¤§æ¨¡å‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ'''



#2ã€å› ä¸ºå®é™…äº§ç”Ÿçš„sqlæ˜¯å½¢å¦‚```sql....```çš„ï¼Œæ— æ³•ç›´æ¥æ‰§è¡Œï¼Œæ‰€ä»¥éœ€è¦æ¸…ç†
#è‡ªå®šä¹‰ä¸€ä¸ªè¾“å‡ºè§£æå™¨SQLCleaner
class SQLCleaner(StrOutputParser):
    def parse(self, text: str) -> str:
        pattern = r'```sql(.*?)```'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            sql = match.group(1).strip()
            # æŸäº›å¤§æ¨¡å‹è¿˜ä¼šäº§ç”Ÿç±»ä¼¼'SQLQuery:'å‰ç¼€ï¼Œå¿…é¡»å»é™¤
            sql = re.sub(r'^SQLQuery:', '', sql).strip()
            return sql
        # æŸäº›å¤§æ¨¡å‹è¿˜ä¼šäº§ç”Ÿç±»ä¼¼'SQLQuery:'å‰ç¼€ï¼Œå¿…é¡»å»é™¤
        text = re.sub(r'^SQLQuery:', '', text).strip()
        return text


#1ã€ç”¨LangChainå†…ç½®é“¾create_sql_query_chainå°†å¤§æ¨¡å‹å’Œæ•°æ®åº“ç»“åˆï¼Œä¼šäº§ç”Ÿsqlè€Œä¸ä¼šæ‰§è¡Œsql
# é€šè¿‡create_sql_query_chainå°†æ­¥éª¤ä¸­çš„1ã€2ã€3åˆèµ·æ¥ä¸€èµ·åšäº†
# sql_make_chain = create_sql_query_chain(client, db)
# resp = sql_make_chain.invoke({"question":"è¯·ä»å›½å®¶è¡¨ä¸­æŸ¥è¯¢å‡ºChinaçš„æ‰€æœ‰æ•°æ®"})
# print("äº§ç”Ÿçš„SQLè¯­å¥ï¼š",resp)
# print("**"*15)
# é¢„å®šä¹‰çš„é“¾ï¼Œé€šç”¨çš„åŠŸèƒ½ï¼Œlangchainå·²ç»å°†å®ç°æµç¨‹å›ºå®šï¼Œä»£ç å·²ç»å®šä¹‰å¥½
sql_make_chain = create_sql_query_chain(client, db)| SQLCleaner()

resp = sql_make_chain.invoke({"question":"è¯·ä»å›½å®¶è¡¨ä¸­æŸ¥è¯¢å‡ºChinaçš„ç›¸å…³æ•°æ®"})
print("å®é™…å¯ç”¨SQL: ",resp)
# print("**"*15)


#3ã€å°†å‰é¢çš„éƒ¨åˆ†ç»„åˆèµ·æ¥ï¼Œå¾—åˆ°æœ€ç»ˆç»“æœ
answer_prompt = PromptTemplate.from_template(
    """ç»™å®šä»¥ä¸‹ç”¨æˆ·é—®é¢˜ã€å¯èƒ½çš„SQLè¯­å¥å’ŒSQLæ‰§è¡Œåçš„ç»“æœï¼Œå›ç­”ç”¨æˆ·é—®é¢˜
    Question: {question}
    SQL Query: {query}
    SQL Result:{result}
    å›ç­”:"""
)
#åˆ›å»ºä¸€ä¸ªæ‰§è¡ŒSQLçš„å·¥å…·
execute_sql_tools = QuerySQLDatabaseTool(db = db)
# runnable = RunnablePassthrough.assign(query=sql_make_chain)
# print("RunnablePassthrough-1ï¼š",runnable.invoke({"question":"è¯·ä»å›½å®¶è¡¨ä¸­æŸ¥è¯¢å‡ºChinaçš„ç›¸å…³æ•°æ®"}))
#
# runnable = RunnablePassthrough.assign(query=sql_make_chain)| itemgetter('query')
# print("RunnablePassthrough-2ï¼š",runnable.invoke({"question":"è¯·ä»å›½å®¶è¡¨ä¸­æŸ¥è¯¢å‡ºChinaçš„ç›¸å…³æ•°æ®"}))
#
# runnable = RunnablePassthrough.assign(query=sql_make_chain)| itemgetter('query') | execute_sql_tools
# print("RunnablePassthrough-3ï¼š",runnable.invoke({"question":"è¯·ä»å›½å®¶è¡¨ä¸­æŸ¥è¯¢å‡ºChinaçš„ç›¸å…³æ•°æ®"}))
#
# runnable = RunnablePassthrough.assign(query=sql_make_chain).assign(result=itemgetter('query')|execute_sql_tools)
# print("RunnablePassthrough-4ï¼š",runnable.invoke({"question":"è¯·ä»å›½å®¶è¡¨ä¸­æŸ¥è¯¢å‡ºChinaçš„ç›¸å…³æ•°æ®"}))
#
# exit()
'''é€šè¿‡ä¸Šé¢çš„æ­¥éª¤ï¼Œå°±èƒ½ææ¸…æ¥š{question}ã€{query}ã€{result}è¿™ä¸‰ä¸ªå­—æ®µæ˜¯å¦‚ä½•é€šè¿‡LCELé“¾ä¸€æ­¥æ­¥è·å¾—çš„
è¦æ³¨æ„çš„æ˜¯result=itemgetter('query')|execute_sql_tools ä¸­ï¼Œæ‰§è¡Œé¡ºåºæ˜¯ï¼š
itemgetter('query') -> execute_sql_tools -> result=
æ‰€ä»¥è¿™æ®µä»£ç å®é™…æ˜¯ï¼šresult=(itemgetter('query')|execute_sql_tools)'''
chain = (RunnablePassthrough.assign(query=sql_make_chain).assign(result=itemgetter('query')|execute_sql_tools)
        |answer_prompt| client| StrOutputParser())


result = chain.invoke(input={"question":"è¯·ä»å›½å®¶è¡¨ä¸­æŸ¥è¯¢å‡ºChinaçš„ç›¸å…³æ•°æ®"})
#result = chain.invoke(input={"question":"è¯·é—®å›½å®¶è¡¨ä¸­æœ‰å¤šå°‘æ¡æ•°æ®"})
print("æœ€ç»ˆæ‰§è¡Œçš„ç»“æœï¼š",result)
'''ï¼Œå¦‚æœåœºæ™¯æ˜¯ç¡®å®šçš„ï¼Œå¹¶ä¸éœ€è¦å¤§æ¨¡å‹æ¥å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·ï¼Œç›´æ¥åœ¨é“¾ä¸­åŠ å…¥å·¥å…·å³å¯
#ä½†æ˜¯å¦‚æœéœ€è¦å¤§æ¨¡å‹æ¥å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·ï¼Œæ¯”å¦‚åœºæ™¯æ˜¯åŠ¨æ€çš„æˆ–è€…æ˜¯ä»¥å·¥å…·ç»„çš„å½¢å¼æä¾›å·¥å…·ï¼Œé‚£ä¹ˆéœ€è¦ä½¿ç”¨Function Callï¼š'''
```



## å…«ã€ä¸­é—´ä»¶

### 1.ä»€ä¹ˆæ˜¯ä¸­é—´ä»¶Middlewareï¼Ÿ

æ˜¯ä¸€ç§ç»†ç²’åº¦æµç¨‹æ§åˆ¶æœºåˆ¶ï¼Œç”¨äºåœ¨æ™ºèƒ½ä½“æ‰§è¡Œè¿‡ç¨‹ä¸­æ‹¦æˆªã€ä¿®æ”¹æˆ–å¢å¼ºè¯·æ±‚ä¸å“åº”çš„å¤„ç†é€»è¾‘ï¼Œè€Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒ Agent æˆ–å·¥å…·çš„ä»£ç ã€‚å¯ä»¥åœ¨è°ƒç”¨æ¨¡å‹æˆ–è€…å·¥å…·æˆ–è€…Agentçš„å‰åè¿›è¡Œæ‹¦æˆªï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€æ±‚åªåšå‰ç½®æ‹¦æˆªæˆ–è€…åç½®æ‹¦æˆª

![1766131171673](C:\Users\EDY\AppData\Roaming\Typora\typora-user-images\1766131171673.png)

### 2.å†…ç½®ä¸­é—´ä»¶

Langchainä¸­å†…ç½®çš„ä¸­é—´ä»¶å¾ˆå¤šï¼Œä»¥ä¸‹åˆ—å‡ºäº†ä¸€éƒ¨åˆ†ä¸­é—´ä»¶çš„ä»‹ç»ï¼š

![1766131352289](C:\Users\EDY\AppData\Roaming\Typora\typora-user-images\1766131352289.png)



ä»¥ä¸‹æˆ‘ä»¬å°±ä»¥SummarizationMiddlewareä¸ºä¾‹ç»™å¤§å®¶è®²è®²ä¸­é—´ä»¶åœ¨Agentä¸­æ€ä¹ˆä½¿ç”¨ï¼ŒSummarizationMiddlewareæ˜¯æ€»ç»“æ‘˜è¦ä¸­é—´ä»¶ï¼ˆä¸Šä¸‹æ–‡å‹ç¼©ï¼‰ï¼Œä»–çš„ä½œç”¨æ˜¯å½“æ¥è¿‘ä¼šè¯æ¬¡æ•°ä¸Šé™æ—¶ï¼Œè‡ªåŠ¨æ±‡æ€»å¯¹è¯å†å²è®°å½•ã€‚

```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
from langgraph.checkpoint.memory import InMemorySaver

from models import *

# Middleware ä¸­é—´ä»¶
# æ·»åŠ ä¸­é—´ä»¶çš„æ–¹å¼ï¼šåœ¨create_agent
'''
agent = create_agent(
    model=qwen_model,
    tools=[],
    middleware=[SummarizationMiddleware(), HumanInTheLoopMiddleware()],
)
'''

# å†…ç½®ä¸­é—´ä»¶
#   LangChain ä¸ºå¸¸è§ç”¨ä¾‹æä¾›é¢„æ„å»ºçš„ä¸­é—´ä»¶ï¼š

# SummarizationMiddleware : æ€»ç»“æ‘˜è¦çš„ä¸­é—´ä»¶
#       å½“æ¥è¿‘ä¼šè¯æ¬¡æ•°ä¸Šé™æ—¶ï¼Œè‡ªåŠ¨æ±‡æ€»å¯¹è¯å†å²è®°å½•ã€‚
#  éå¸¸é€‚åˆï¼š
#     - æŒç»­æ—¶é—´è¿‡é•¿çš„å¯¹è¯è¶…å‡ºäº†ä¸Šä¸‹æ–‡çª—å£ã€‚
#     - å¤šè½®å¯¹è¯ï¼Œå†å²æ‚ ä¹…
#     - åœ¨éœ€è¦ä¿ç•™å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡çš„åº”ç”¨ä¸­
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

llm = ChatOpenAI(
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    openai_api_key=os.getenv("DASHSCOPE_API_KEY"),
    model_name="qwen-max"
)
# åˆ›å»ºçŸ­æœŸè®°å¿†å®ä¾‹
memory = InMemorySaver()
agent = create_agent(
    model=llm,
    tools=[],
    checkpointer=memory,
    # ä¸­é—´ä»¶åˆ—è¡¨ï¼Œå¯ä»¥å¤šä¸ªï¼Œå¤šä¸ªé¡ºåºæ‰§è¡Œ
    middleware=[
        SummarizationMiddleware(
            model=llm,
            max_tokens_before_summary=80,  # 80ä¸ªtoken ä¼šè§¦å‘ æ‘˜è¦æ€»ç»“
            messages_to_keep=1,  # åœ¨æ€»ç»“åä¿ç•™æœ€å1æ¡æ¶ˆæ¯
            # å¯é€‰ summary_prompt=" å¯ä»¥è‡ªå®šä¹‰è¿›è¡Œæ‘˜è¦çš„æç¤ºè¯...",
            summary_prompt="è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²è¿›è¡Œç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯: {messages}"
        ),
    ],
    # æ‰“å°Agentæ‰§è¡Œçš„è¿‡ç¨‹æ—¥å¿—
    debug= True
)

# å•ä¸€æ¡ä»¶:å½“tokens> = 4000ä¸”æ¶ˆæ¯> = 10æ—¶è§¦å‘
# agent = create_agent(
#     model=llm,
#     tools=[weather_tool, add_tool],
#     middleware=[
#         SummarizationMiddleware(
#             model=llm_other,
#             trigger={"tokens": 4000, "messages": 10},
#             keep={"messages": 20},
#         ),
#     ],
# )
#
# # å¤šé‡æ¡ä»¶-ï¼ˆä»»æ„æ¡ä»¶å¿…é¡»æ»¡è¶³ - é€»è¾‘â€œæˆ–â€ï¼‰ã€‚
# agent2 = create_agent(
#     model="gpt-4o",
#     tools=[weather_tool, add_tool],
#     middleware=[
#         SummarizationMiddleware(
#             model="gpt-4o-mini",
#             trigger=[
#                 {"tokens": 5000, "messages": 3},
#                 {"tokens": 3000, "messages": 6},
#             ],
#             keep={"messages": 20},
#         ),
#     ],
# )

# æ¨¡æ‹Ÿé•¿å¯¹è¯è§¦å‘æ‘˜è¦
print("\næ¨¡æ‹Ÿé•¿å¯¹è¯åœºæ™¯...")
demo_messages = [
    "ç”¨æˆ·è¯¢é—®ä½ æ˜¯è°",
    "ç”¨æˆ·è®¡ç®—å•†å“ä»·æ ¼ï¼šæ•°é‡10ï¼Œå•ä»·25.5",
    "ç”¨æˆ·å†æ¬¡è¯¢é—®ä½ èƒ½åšä»€ä¹ˆï¼Ÿ",
    "ç”¨æˆ·æƒ³è¦ç”Ÿæˆä¸€ä¸ªä»‹ç»æ¹–å—çš„æ–‡æ¡ˆï¼Œè¦æ±‚100å­—å·¦å³ï¼ŒåŒ…å«ä¸‰æ¹˜å››æ°´ï¼Œäººæ–‡å†å²",
    "ç”¨æˆ·ç»§ç»­è¯¢é—®æ›´å¤šGPUäº§å“ä¿¡æ¯",
    "ç”¨æˆ·è¦æ±‚è®¡ç®—2*20"
]

for i, message in enumerate(demo_messages, 1):
    print(f"\nğŸ’¬ ç¬¬{i}è½®å¯¹è¯: {message}")
    # å¾ªç¯è°ƒç”¨Agentï¼Œæ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    result = agent.invoke({
        "messages": [HumanMessage(content=message)]},
        config={"configurable": {"thread_id": "testsummarizationMiddleware"}}
    )
    # print("æ‰§è¡Œç»“æœï¼š"+result)
```

### 3.è‡ªå®šä¹‰ä¸­é—´ä»¶

![1766131635744](C:\Users\EDY\AppData\Roaming\Typora\typora-user-images\1766131635744.png)

å…·ä½“è¯¦ç»†æ¡ˆä¾‹å‚è€ƒè¯¾ç¨‹ä¸­çš„ä»£ç 